
☐ fairseq debug loadding files in denoising task
☐ fairseq mbart50 load
☐ fairseq mbart50 train script and finetune script
check why the extended dict can load a original model

PYTORCH_FAIRSEQ_CACHE: /home/xianjiay/.cache/torch/pytorch_fairseq

append_eos = False

language without segment should take "en_XX" as input

begining of words: add more special tokens


[en_XX] [KG] [ENT] ▁Romania [TRIPLE] [PRED] ▁description [SUB] ▁Romania [TRIPLE] [PRED] ▁country [SUB] ▁Alba ▁Iulia [TRIPLE]

1 masking tokens
2 masking entities
3 masking sub content

    in trainer:
    epoch_itr = trainer.get_train_iterator()

    trainer.get_train_iterator:
        batch_iterator = self.task.get_batch_iterator()
        return batch_iterator

    task.get_batch_iterator() in Fairseq_task:
        batch_sampler = dataset.batch_by_size() requires 1
        [len(multiples of required), index]
        epoch_iter = iterators.EpochBatchIterator() requires 2
        return epoch_iter

    dataset.batch_by_size() in fairseq_dataset:
        return data_utils.batch_by_size()

    fairseq.data.iterators.EpochBatchIterator class init

    in train:
        itr = epoch_itr.next_epoch_itr()
    in fairseq.data.iterators.EpochBatchIterator.next_epoch_itr():
    self.dataset.set_epoch(self.epoch)
    self._cur_epoch_itr = self._get_iterator_for_epoch(
            self.epoch,
            shuffle,
            fix_batches_to)
    in fairseq.data.iterators.EpochBatchIterator._get_iterator_for_epoch():
        itr = torch.utils.data.DataLoader(
            self.dataset,
            collate_fn=self.collate_fn,
            batch_sampler=batches[offset:],
            num_workers=self.num_workers,
            timeout=self.timeout,
        )

        requires dataset.set_epoch, collate_fn
        return itr
    
        progress_bar
    in fairseq.logging.progress_bar.progress_bar
    ??
    in trainer.train_step:
        self.task.train_step(
                        sample=sample,
                        model=self.model,
                        criterion=self.criterion,
                        optimizer=self.optimizer,
                        update_num=self.get_num_updates(),
                        ignore_grad=is_dummy_batch,
                    )
        -> fairseqTask.train_step
    in fairseqTask.train_step:
        loss, sample_size, logging_output = criterion(model, sample)
    in fairseq.criterions.LabelSmoothedCrossEntropyCriterion(FairseqCriterion):
        net_output = model(**sample["net_input"])

    * sample:(len=6)
        "id": tensor()
        "ntokens": 888
        "net_input":
            "src_tokens": why *2?
            "src_lengths": [37,37,37, ....]
            "prev_output_tokens"
        "target":
        "nsentences": 37
        "sort_order": tensor




requires 1:
    Dataset:
    def num_tokens(self, index: int):
        return np.max(self.size(index))
    
    def size(self, idx: int):
        """
        Return an example's size as a float or tuple.
        """
        dataset_idx, sample_idx = self._get_dataset_and_sample_index(idx)
        return self.datasets[dataset_idx].size(sample_idx)

    def _get_dataset_and_sample_index(self, idx: int):
        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1] # 多个dataset 且超出第一个的index
        sample_idx = sample_idx % self.real_sizes[dataset_idx]
        return dataset_idx, sample_idx

    Resampling dataset:

    def size(self, index): # index in sampling -> true index -> call original size method to return size for tru index
        # -> and size() in concat_dataset actually calls num_tokens() method
        return self.dataset.size(self._cur_indices.array[index])

requires 2:
    parameters:
        collate_fn=dataset.collater
        batch_sampler=batch_sampler
    dataset (concat_dataset)
    @property
    def supports_prefetch(self):
        return all(d.supports_prefetch for d in self.datasets)

requires 3:
    train:
        dataset.set_epoch (-> BaseWrapperDataset -> FairseqDataset -> concat_dataset?? 注意继承)






