common:
  # common
  logging_steps: 20
  printing_steps: 500
  save_every_n_epochs: 1
  # output
  output_dir: checkpoint # check!
  loss_log_file: ""
  res_file: ""
  # training
  epochs: 10  # task.data
  starting_epoch: 0
  learning_rate: 1e-4 # task.data.task.learning_rate
  finetune: True
  beam_size: 2
  bleu: 4
  copy_loss: False
  device: gpu
  seed: 1
  tensorboard_logdir: ""
  # task
  # need to implement: 'train', 'test', 'challenge', 'visualize', 'LM', 'compute_bleu', 'few_shot'
  percent: 1.0
  # data
  # additional: "" for few shot log name
task:
  # train/few_shot/test  if set few_shot, specify percent
  # finetune: set finetune: true and option: train  train: set option: train and finetune: False
  option: train
  dataset: wikidata
  additional: "" # task.data
  percent: 1.0
  finetune: False
  tokenizer:
    tokenizer_dir: "/GPT2_tokenizer/"
    vocab_size: 50264
    sos_token_id: 50262
    eos_token_id: 50261
    pad_token_id: 50263
    bos_token: "[SOS]"
    eos_token: "[EOS]"
    hidden_size: 768
    config: "/GPT2_tokenizer/knowledge_config.json"

  data:
    wikidata:
      dataset: wikidata
      encoder: "sequence"
      lower_case: False
      train_file_path: "/dataset/wikidata/train.json"
      eval_file_path: "/dataset/wikidata/eval.json"
      test_file_path: "/dataset/wikidata/test.json"
      max_entity: 12
      max_fact: 8
      forbid_duplicate_relation: true
      knowledge_path: "/preprocess/knowledge-full.json"
      max_enc_len: 256
      max_dec_len: 72
      train:
        num_workers: 8
        batch_size: 128
        epochs: 100
        printing_steps: 200
        save_every_n_epochs: 10
        max_enc_len: 256
        max_dec_len: 72
        learning_rate: 1e-4

    webnlg:
      dataset: webnlg
      encoder: "sequence"
      lower_case: False
      train_file_path: "/dataset/webnlg/train.json"
      eval_file_path: "/dataset/webnlg/eval.json"
      test_file_path: "/dataset/webnlg/test.json"
      max_entity: 12
      max_fact: 8
      forbid_duplicate_relation: true
      knowledge_path: "/preprocess/knowledge-full.json"
      max_enc_len: 256
      max_dec_len: 72
      train:
        num_workers: 8
        batch_size: 64 # default 64
        epochs: 100
        printing_steps: 200
        save_every_n_epochs: 1 #default 10
        max_enc_len: 256
        max_dec_len: 72
        learning_rate: 1e-4
      eval:
        num_workers: 0
        batch_size: 16
        printing_steps: 500
        beam_size: 2
        save_every_n_epochs: 1
        max_enc_len: 256 #325
        max_dec_len: 72
      finetune:
        num_workers: 8
        batch_size: 64 # default 64
        epochs: 100
        printing_steps: 200
        save_every_n_epochs: 10
        max_enc_len: 256
        max_dec_len: 72
        learning_rate: 4e-5
      few_shot:
        num_workers: 8
        batch_size: 64 #default: 64
        epochs: 200 # default 200
        printing_steps: 200
        save_every_n_epochs: 40
        max_enc_len: 256
        max_dec_len: 72
        learning_rate: 5e-5

    wikibionlg:
      dataset: "wikibionlg"
      encoder: "sequence"
      lower_case: False
      train_file_path: "/dataset/wikibionlg/train.json"
      eval_file_path: "/dataset/wikibionlg/eval.json"
      test_file_path: "/dataset/wikibionlg/test.json"
      max_entity: 12 #?
      max_fact: 8 #?
      forbid_duplicate_relation: true #?
      knowledge_path: "/preprocess/knowledge-full.json"
      max_enc_len: 425
      max_dec_len: 72
      train:
        num_workers: 8
        batch_size: 64 #default 64
        epochs: 20
        printing_steps: 200
        save_every_n_epochs: 1
        max_enc_len: 425
        max_dec_len: 72
        learning_rate: 1e-4
      eval:
        num_workers: 4
        batch_size: 64
        beam_size: 2
        max_enc_len: 425
        max_dec_len: 72
        printing_steps: 200
      finetune:
        num_workers: 8
        batch_size: 64
        epochs: 20
        printing_steps: 200
        save_every_n_epochs: 1
        max_enc_len: 425
        max_dec_len: 72
        learning_rate: 2e-5
      few_shot:
        batch_size: 64 #default: 64
        num_workers: 8
        epochs: 400
        printing_steps: 200
        save_every_n_epochs: 200
        max_enc_len: 425
        max_dec_len: 72
        learning_rate: 1e-4

  model:
    kgpt:
      load_from: "" # pretrain: model_ep14.pt
      max_entity_embeddings: 30
      max_triple_embeddings: 20
      max_position_embeddings: 1024
      layer_norm_eps: 1e-12

      # encoder
      hidden_size_enc: 768
      hidden_dropout_prob_enc: 0.1
      pad_token_id_src: 50263
      n_layers_enc: 6
      n_heads_enc: 8
      embedding_path_enc: ""

      # decoder
      hidden_size_dec: 768
      hidden_dropout_prob_dec: 0.1
      pad_token_id_tgt: 50263
      n_layers_dec: 6
      n_heads_dec: 8
      embedding_path_dec: ""
kg2text_mini:
  CommonConfig:
    # saving results config here
    log_interval: 20 # (stpes, logging_steps)
    user_dir: media/MyDataStor1/jxian/efs-storage/Kg2text
  DistributedTrainingConfig:
    device_id: 0
  DatasetConfig:
    num_workers: 1
    batch_size: 64
    train_subset: "train"
    valid_subset: "valid"
  OptimizationConfig:
    max_epoch: 0
    stop_min_lr: -1.0
  CheckpointConfig:
    save_dir: "checkpoints"
    restore_file: "checkpoint_last.pt"
    finetune_from_model:
    save_interval
    keep_best_checkpoints: 5 # number of checkpoints to keep

